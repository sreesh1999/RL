{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow==2.15.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFpnj5K-Ial-","outputId":"814948d3-9171-4019-e3c1-c6ef0954fb03","executionInfo":{"status":"ok","timestamp":1724063751688,"user_tz":-330,"elapsed":83534,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.15.0\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n","  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.16.0\n","    Uninstalling wrapt-1.16.0:\n","      Successfully uninstalled wrapt-1.16.0\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.0\n","    Uninstalling ml-dtypes-0.4.0:\n","      Successfully uninstalled ml-dtypes-0.4.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"]}]},{"cell_type":"markdown","source":["**Sepsis Environment**"],"metadata":{"id":"hhUutyH6NOjY"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"iTCikNRp95My","executionInfo":{"status":"ok","timestamp":1724065177186,"user_tz":-330,"elapsed":443,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","# Define the columns for state features and action features\n","state_cols = ['SOFA']  # Example state columns\n","action_cols = ['MaxVaso', 'Input4H']  # Medication columns\n","\n","def predict_medication_effects(model, state, iv_fluid_dosage, vp_dosage, history):\n","    current_state = state[state_cols].values.reshape(1, -1)  # Shape: (1, 1) if 'SOFA' is the only state feature\n","    action = np.array([vp_dosage, iv_fluid_dosage]).reshape(1, -1)  # Shape: (1, 2)\n","    print (action)\n","\n","    # Concatenate current state and action into a single input array\n","    model_input = np.concatenate([current_state, action], axis=1)  # Shape: (1, 3)\n","\n","    # Concatenate historical cases with current input\n","    model_input = np.concatenate([model_input, history.reshape(1, -1)], axis=1)  # Shape: (1, 12)\n","\n","    # Predict the next state\n","    state_change = model.predict(model_input)\n","\n","    # Update the state with the predicted changes\n","    next_state = state.copy()\n","    next_state[state_cols] += state_change[0]\n","\n","    return next_state, state_change[0][0]\n","\n","class SepsisEnv(gym.Env):\n","    def __init__(self, dataset, model_path,action_history_input):\n","        super(SepsisEnv, self).__init__()\n","        self.dataset = dataset\n","        self.model = tf.keras.models.load_model(model_path)  # Load the trained model\n","        self.current_index = 0\n","        self.history_size = 3  # Size of history to maintain\n","        self.action_history = np.array(action_history_input)  # Initialize with example history\n","        self.action_space = spaces.Discrete(25)  # 5 x 5 = 25 possible actions\n","\n","        # Calculate observation space size\n","        self.observation_size = len(state_cols) + len(action_cols) + len(state_cols) * self.history_size\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.observation_size,), dtype=np.float32)\n","\n","    def reset(self):\n","        self.current_index = np.random.randint(0, len(self.dataset))\n","        self.action_history = np.array(action_history_input)  # Reset to example history\n","        return self._get_observation()\n","\n","    def _get_observation(self):\n","        state = self.dataset.iloc[self.current_index][state_cols].values\n","        current_action = self.dataset.iloc[self.current_index][action_cols].values\n","        recent_history = self.action_history.flatten()  # Use action history\n","\n","        return np.concatenate((state, current_action, recent_history))\n","\n","    def step(self, action,time):\n","        state = self.dataset.iloc[self.current_index].copy()\n","        next_state = state.copy()\n","\n","        dosage_list=self.decode_dosage(action)\n","        iv_fluid_dosage = dosage_list[1]\n","        vp_dosage = dosage_list[0]\n","\n","        next_state, sofa_change = predict_medication_effects(self.model, next_state, iv_fluid_dosage, vp_dosage, self.action_history)\n","\n","        current_sofa = self.action_history[2][0]\n","        next_sofa = sofa_change\n","\n","        if next_sofa >=25 or next_sofa <6 :\n","            done = True\n","        else:\n","            done = False\n","\n","        reward = self.calculate_reward(current_sofa, next_sofa, done,time)\n","\n","        # Update action history with current action (vp_dosage, iv_fluid_dosage)\n","        self.action_history = np.roll(self.action_history, -1, axis=0)  # Remove the oldest entry\n","        self.action_history[-1] = [sofa_change, vp_dosage, iv_fluid_dosage]  # Add new action at the end\n","        print(\"Action history:\", self.action_history)\n","\n","        print(\"Reward:\",reward)\n","        print(\"Next SOFA:\", next_sofa)\n","        print(\"Current SOFA:\", current_sofa)\n","\n","\n","\n","        self.current_index += 1\n","\n","        print(done)\n","        # Prepare the observation to return\n","        observation = self._get_observation()\n","\n","        # Return observation, reward, done status, and info dictionary\n","        info = {'predicted_sofa_state': next_state, 'action_applied': action}\n","        return observation, reward, done, info\n","\n","    def calculate_reward(self, current_sofa, next_sofa, done, time):\n","        # Immediate reward based on SOFA score change\n","        if next_sofa < current_sofa:\n","            sofa_reward = (current_sofa - next_sofa) * 2\n","        elif next_sofa > current_sofa:\n","            sofa_reward = (next_sofa - current_sofa) * -2\n","        else:\n","            sofa_reward = 1  # Small reward for maintaining the SOFA score\n","\n","        # Terminal reward based on episode end and SOFA score\n","        if done:\n","            if next_sofa <= 5:\n","                terminal_reward = 20  # High reward for achieving a low SOFA score at the end\n","                # Bonus reward for achieving the goal in fewer steps\n","                step_reward = (40 - time) / 40 * 10  # Adjust the multiplier as needed\n","            else:\n","                terminal_reward = -10  # Penalty for high SOFA score at the end\n","                step_reward = 0  # No step reward if the goal is not achieved\n","        else:\n","            terminal_reward = 0\n","            step_reward = 0  # No step reward if the episode is not done\n","\n","        # Delayed reward: Smaller rewards for staying alive and progressing through time steps\n","        # survival_reward = (40 - time) / 40\n","\n","        return sofa_reward + terminal_reward + step_reward\n","\n","\n","    def decode_dosage(self, dosage):\n","        vp_dosages = [0, 0.001, 0.01, 0.1, 1]\n","        iv_fluid_dosages = [0, 20, 60, 100, 200]\n","\n","        flattened_array = [\n","                                    [0, 0], [0, 20], [0, 60], [0, 100], [0, 200],\n","                                    [0.001, 0], [0.001, 20], [0.001, 60], [0.001, 100], [0.001, 200],\n","                                    [0.01, 0], [0.01, 20], [0.01, 60], [0.01, 100], [0.01, 200],\n","                                    [0.1, 0], [0.1, 20], [0.1, 60], [0.1, 100], [0.1, 200],\n","                                    [1, 0], [1, 20], [1, 60], [1, 100], [1, 200]\n","                                ]\n","\n","\n","            # Function to access data by a single index (0-24)\n","\n","        return flattened_array[dosage]\n","\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Check TensorFlow version\n","print(\"TensorFlow version:\", tf.__version__)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owY5PDDTIwEG","outputId":"1f72def3-8685-4a18-be1b-551d9416c423","executionInfo":{"status":"ok","timestamp":1724065205252,"user_tz":-330,"elapsed":491,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.15.0\n"]}]},{"cell_type":"markdown","source":["**DDQN Agent**"],"metadata":{"id":"mqM8eaR8NX7B"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","import random\n","\n","# Load dataset\n","dataset_path = '/content/drive/MyDrive/RL project/Dataset.csv'\n","dataset = pd.read_csv(dataset_path)\n","action_history_input = []\n","\n","# Number of entries you want to input\n","num_entries = 3\n","\n","# Loop to input values\n","for i in range(num_entries):\n","    sofa_score = int(input(f\"Enter SOFA score for entry {i + 1}: \"))\n","    max_vaso = int(input(f\"Enter Max Vaso dosage for entry {i + 1}: \"))\n","    iv_fluid = int(input(f\"Enter IV fluid dosage for entry {i + 1}: \"))\n","    action_history_input.append([sofa_score, max_vaso, iv_fluid])\n","# Select 10% of the dataset\n","\n","# Initialize environment\n","env = SepsisEnv(dataset, '/content/drive/MyDrive/RL project/predict_state_model.keras',action_history_input)\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","class DDQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.learning_rate = 0.001\n","        self.epsilon = 1.0  # exploration rate\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        self.model = self._build_model()\n","\n","\n","\n","    def _build_model(self):\n","        model = Sequential()\n","        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n","        model.add(Dense(24, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","        return model\n","\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state)\n","        print (action)\n","\n","        return np.argmax(act_values[0])  # returns action\n","\n","\n","\n","\n","\n","# Initialize DDQN agent\n","agent = DDQNAgent(state_size, action_size)\n","\n","# Load trained model weights\n","agent.model.load_weights(\"/content/drive/MyDrive/RL project/ddqn_sepsis_170.h5\")\n","\n","# Run inference\n","state = env.reset()\n","state = np.reshape(state, [2, state_size])  # Ensure the state has the correct shape\n","total_reward = 0\n","done = False\n","time_steps = 40 # Define the number of time steps for each episode\n","time=0\n","while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, info = env.step(action,time=time)\n","    time += 1\n","    next_state = np.reshape(next_state, [2, state_size])  # Ensure the next_state has the correct shape\n","    state = next_state\n","    total_reward += reward\n","    print(f\"Action: {action}, Reward: {reward}, Total Reward: {total_reward}\")\n","\n","print(f\"Total Reward after episode: {total_reward}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wkb5PD1N-NAE","outputId":"7bd29ed5-2765-472d-bf81-26dd723824df","executionInfo":{"status":"ok","timestamp":1724065309744,"user_tz":-330,"elapsed":101550,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter SOFA score for entry 1: 10\n","Enter Max Vaso dosage for entry 1: 0\n","Enter IV fluid dosage for entry 1: 0\n","Enter SOFA score for entry 2: 10\n","Enter Max Vaso dosage for entry 2: 0\n","Enter IV fluid dosage for entry 2: 0\n","Enter SOFA score for entry 3: 19\n","Enter Max Vaso dosage for entry 3: 0\n","Enter IV fluid dosage for entry 3: 0\n","[[ 0 60]]\n","1/1 [==============================] - 0s 88ms/step\n","Action history: [[10  0  0]\n"," [19  0  0]\n"," [11  0 60]]\n","Reward: 15.274435043334961\n","Next SOFA: 11.3627825\n","Current SOFA: 19\n","False\n","Action: 2, Reward: 15.274435043334961, Total Reward: 15.274435043334961\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[19  0  0]\n"," [11  0 60]\n"," [10  0 20]]\n","Reward: 1.4135150909423828\n","Next SOFA: 10.293242\n","Current SOFA: 11\n","False\n","Action: 16, Reward: 1.4135150909423828, Total Reward: 16.687950134277344\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[11  0 60]\n"," [10  0 20]\n"," [14  0 20]]\n","Reward: -8.770832061767578\n","Next SOFA: 14.385416\n","Current SOFA: 10\n","False\n","Action: 11, Reward: -8.770832061767578, Total Reward: 7.917118072509766\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[10  0 20]\n"," [14  0 20]\n"," [11  0  0]]\n","Reward: 5.398538589477539\n","Next SOFA: 11.300731\n","Current SOFA: 14\n","False\n","Action: 10, Reward: 5.398538589477539, Total Reward: 13.315656661987305\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[14  0 20]\n"," [11  0  0]\n"," [10  0 20]]\n","Reward: 1.9190807342529297\n","Next SOFA: 10.04046\n","Current SOFA: 11\n","False\n","Action: 16, Reward: 1.9190807342529297, Total Reward: 15.234737396240234\n","[[0 0]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[11  0  0]\n"," [10  0 20]\n"," [12  0  0]]\n","Reward: -4.944158554077148\n","Next SOFA: 12.472079\n","Current SOFA: 10\n","False\n","Action: 0, Reward: -4.944158554077148, Total Reward: 10.290578842163086\n","[[1 0]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[10  0 20]\n"," [12  0  0]\n"," [ 9  1  0]]\n","Reward: 4.517292022705078\n","Next SOFA: 9.741354\n","Current SOFA: 12\n","False\n","Action: 20, Reward: 4.517292022705078, Total Reward: 14.807870864868164\n","[[ 0 20]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[12  0  0]\n"," [ 9  1  0]\n"," [10  0 20]]\n","Reward: -2.8522396087646484\n","Next SOFA: 10.42612\n","Current SOFA: 9\n","False\n","Action: 1, Reward: -2.8522396087646484, Total Reward: 11.955631256103516\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[ 9  1  0]\n"," [10  0 20]\n"," [10  0  0]]\n","Reward: -1.9129104614257812\n","Next SOFA: 10.956455\n","Current SOFA: 10\n","False\n","Action: 10, Reward: -1.9129104614257812, Total Reward: 10.042720794677734\n","[[ 1 20]]\n","1/1 [==============================] - 0s 27ms/step\n","Action history: [[10  0 20]\n"," [10  0  0]\n"," [ 8  1 20]]\n","Reward: 2.9150543212890625\n","Next SOFA: 8.542473\n","Current SOFA: 10\n","False\n","Action: 21, Reward: 2.9150543212890625, Total Reward: 12.957775115966797\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 10   0   0]\n"," [  8   1  20]\n"," [ 10   0 100]]\n","Reward: -4.238130569458008\n","Next SOFA: 10.119065\n","Current SOFA: 8\n","False\n","Action: 8, Reward: -4.238130569458008, Total Reward: 8.719644546508789\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[  8   1  20]\n"," [ 10   0 100]\n"," [  9   0  60]]\n","Reward: 0.6712303161621094\n","Next SOFA: 9.664385\n","Current SOFA: 10\n","False\n","Action: 7, Reward: 0.6712303161621094, Total Reward: 9.390874862670898\n","[[0 0]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 10   0 100]\n"," [  9   0  60]\n"," [  8   0   0]]\n","Reward: 1.3549385070800781\n","Next SOFA: 8.322531\n","Current SOFA: 9\n","False\n","Action: 0, Reward: 1.3549385070800781, Total Reward: 10.745813369750977\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[ 9  0 60]\n"," [ 8  0  0]\n"," [ 9  0 20]]\n","Reward: -2.241922378540039\n","Next SOFA: 9.120961\n","Current SOFA: 8\n","False\n","Action: 16, Reward: -2.241922378540039, Total Reward: 8.503890991210938\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[ 8  0  0]\n"," [ 9  0 20]\n"," [ 9  0  0]]\n","Reward: -0.08401107788085938\n","Next SOFA: 9.042006\n","Current SOFA: 9\n","False\n","Action: 10, Reward: -0.08401107788085938, Total Reward: 8.419879913330078\n","[[ 0.1 60. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 9  0 20]\n"," [ 9  0  0]\n"," [ 7  0 60]]\n","Reward: 2.1374950408935547\n","Next SOFA: 7.9312525\n","Current SOFA: 9\n","False\n","Action: 17, Reward: 2.1374950408935547, Total Reward: 10.557374954223633\n","[[  1 100]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  9   0   0]\n"," [  7   0  60]\n"," [  8   1 100]]\n","Reward: -3.8029708862304688\n","Next SOFA: 8.901485\n","Current SOFA: 7\n","False\n","Action: 23, Reward: -3.8029708862304688, Total Reward: 6.754404067993164\n","[[ 1 20]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0  60]\n"," [  8   1 100]\n"," [  8   1  20]]\n","Reward: -1.380645751953125\n","Next SOFA: 8.690323\n","Current SOFA: 8\n","False\n","Action: 21, Reward: -1.380645751953125, Total Reward: 5.373758316040039\n","[[  0 100]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  8   1 100]\n"," [  8   1  20]\n"," [  7   0 100]]\n","Reward: 0.3203754425048828\n","Next SOFA: 7.8398123\n","Current SOFA: 8\n","False\n","Action: 3, Reward: 0.3203754425048828, Total Reward: 5.694133758544922\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[  8   1  20]\n"," [  7   0 100]\n"," [  7   0  20]]\n","Reward: -1.94390869140625\n","Next SOFA: 7.9719543\n","Current SOFA: 7\n","False\n","Action: 6, Reward: -1.94390869140625, Total Reward: 3.750225067138672\n","[[  0 200]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  7   0 100]\n"," [  7   0  20]\n"," [  7   0 200]]\n","Reward: -1.408951759338379\n","Next SOFA: 7.704476\n","Current SOFA: 7\n","False\n","Action: 4, Reward: -1.408951759338379, Total Reward: 2.341273307800293\n","[[  1 100]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  7   0  20]\n"," [  7   0 200]\n"," [  7   1 100]]\n","Reward: -1.0744867324829102\n","Next SOFA: 7.5372434\n","Current SOFA: 7\n","False\n","Action: 23, Reward: -1.0744867324829102, Total Reward: 1.2667865753173828\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0 200]\n"," [  7   1 100]\n"," [  7   0   0]]\n","Reward: -0.5705909729003906\n","Next SOFA: 7.2852955\n","Current SOFA: 7\n","False\n","Action: 10, Reward: -0.5705909729003906, Total Reward: 0.6961956024169922\n","[[  0 100]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   1 100]\n"," [  7   0   0]\n"," [  7   0 100]]\n","Reward: -1.0818328857421875\n","Next SOFA: 7.5409164\n","Current SOFA: 7\n","False\n","Action: 3, Reward: -1.0818328857421875, Total Reward: -0.3856372833251953\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0   0]\n"," [  7   0 100]\n"," [  7   0 100]]\n","Reward: -1.5940332412719727\n","Next SOFA: 7.7970166\n","Current SOFA: 7\n","False\n","Action: 8, Reward: -1.5940332412719727, Total Reward: -1.979670524597168\n","[[ 0 20]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  7   0 100]\n"," [  7   0 100]\n"," [  7   0  20]]\n","Reward: -0.7162141799926758\n","Next SOFA: 7.358107\n","Current SOFA: 7\n","False\n","Action: 1, Reward: -0.7162141799926758, Total Reward: -2.6958847045898438\n","[[1.e-02 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0 100]\n"," [  7   0  20]\n"," [  7   0 200]]\n","Reward: -0.8192987442016602\n","Next SOFA: 7.4096494\n","Current SOFA: 7\n","False\n","Action: 14, Reward: -0.8192987442016602, Total Reward: -3.515183448791504\n","[[  0 100]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  7   0  20]\n"," [  7   0 200]\n"," [  7   0 100]]\n","Reward: -1.1049585342407227\n","Next SOFA: 7.5524793\n","Current SOFA: 7\n","False\n","Action: 3, Reward: -1.1049585342407227, Total Reward: -4.620141983032227\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  7   0 200]\n"," [  7   0 100]\n"," [  7   0 200]]\n","Reward: -0.1751270294189453\n","Next SOFA: 7.0875635\n","Current SOFA: 7\n","False\n","Action: 19, Reward: -0.1751270294189453, Total Reward: -4.795269012451172\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0 100]\n"," [  7   0 200]\n"," [  7   0 100]]\n","Reward: -0.7539043426513672\n","Next SOFA: 7.376952\n","Current SOFA: 7\n","False\n","Action: 8, Reward: -0.7539043426513672, Total Reward: -5.549173355102539\n","[[ 0.1 60. ]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  7   0 200]\n"," [  7   0 100]\n"," [  7   0  60]]\n","Reward: -0.7406339645385742\n","Next SOFA: 7.370317\n","Current SOFA: 7\n","False\n","Action: 17, Reward: -0.7406339645385742, Total Reward: -6.289807319641113\n","[[1 0]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  7   0 100]\n"," [  7   0  60]\n"," [  6   1   0]]\n","Reward: 0.0014696121215820312\n","Next SOFA: 6.999265\n","Current SOFA: 7\n","False\n","Action: 20, Reward: 0.0014696121215820312, Total Reward: -6.288337707519531\n","[[ 1 20]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[ 7  0 60]\n"," [ 6  1  0]\n"," [ 6  1 20]]\n","Reward: -1.950942039489746\n","Next SOFA: 6.975471\n","Current SOFA: 6\n","False\n","Action: 21, Reward: -1.950942039489746, Total Reward: -8.239279747009277\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 6  1  0]\n"," [ 6  1 20]\n"," [ 7  0 20]]\n","Reward: -3.410186767578125\n","Next SOFA: 7.7050934\n","Current SOFA: 6\n","False\n","Action: 16, Reward: -3.410186767578125, Total Reward: -11.649466514587402\n","[[  1 200]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   1  20]\n"," [  7   0  20]\n"," [  6   1 200]]\n","Reward: 0.43068981170654297\n","Next SOFA: 6.784655\n","Current SOFA: 7\n","False\n","Action: 24, Reward: 0.43068981170654297, Total Reward: -11.21877670288086\n","[[  1 200]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0  20]\n"," [  6   1 200]\n"," [  6   1 200]]\n","Reward: -1.5456733703613281\n","Next SOFA: 6.7728367\n","Current SOFA: 6\n","False\n","Action: 24, Reward: -1.5456733703613281, Total Reward: -12.764450073242188\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  6   1 200]\n"," [  6   1 200]\n"," [  7   0  60]]\n","Reward: -2.8715991973876953\n","Next SOFA: 7.4357996\n","Current SOFA: 6\n","False\n","Action: 7, Reward: -2.8715991973876953, Total Reward: -15.636049270629883\n","[[  1 200]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  6   1 200]\n"," [  7   0  60]\n"," [  6   1 200]]\n","Reward: 0.05669116973876953\n","Next SOFA: 6.9716544\n","Current SOFA: 7\n","False\n","Action: 24, Reward: 0.05669116973876953, Total Reward: -15.579358100891113\n","[[ 0 20]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0  60]\n"," [  6   1 200]\n"," [  7   0  20]]\n","Reward: -2.16220760345459\n","Next SOFA: 7.081104\n","Current SOFA: 6\n","False\n","Action: 1, Reward: -2.16220760345459, Total Reward: -17.741565704345703\n","[[  0 100]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   1 200]\n"," [  7   0  20]\n"," [  7   0 100]]\n","Reward: -0.09990310668945312\n","Next SOFA: 7.0499516\n","Current SOFA: 7\n","False\n","Action: 3, Reward: -0.09990310668945312, Total Reward: -17.841468811035156\n","[[ 0 20]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   0  20]\n"," [  7   0 100]\n"," [  7   0  20]]\n","Reward: -0.18545055389404297\n","Next SOFA: 7.0927253\n","Current SOFA: 7\n","False\n","Action: 1, Reward: -0.18545055389404297, Total Reward: -18.0269193649292\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   0 100]\n"," [  7   0  20]\n"," [  7   0   0]]\n","Reward: -0.6133003234863281\n","Next SOFA: 7.30665\n","Current SOFA: 7\n","False\n","Action: 5, Reward: -0.6133003234863281, Total Reward: -18.640219688415527\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[  7   0  20]\n"," [  7   0   0]\n"," [  7   0 200]]\n","Reward: -0.3939199447631836\n","Next SOFA: 7.19696\n","Current SOFA: 7\n","False\n","Action: 9, Reward: -0.3939199447631836, Total Reward: -19.03413963317871\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0   0]\n"," [  7   0 200]\n"," [  7   0  20]]\n","Reward: -1.151611328125\n","Next SOFA: 7.5758057\n","Current SOFA: 7\n","False\n","Action: 16, Reward: -1.151611328125, Total Reward: -20.18575096130371\n","[[  0 200]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0 200]\n"," [  7   0  20]\n"," [  7   0 200]]\n","Reward: -0.1198883056640625\n","Next SOFA: 7.059944\n","Current SOFA: 7\n","False\n","Action: 4, Reward: -0.1198883056640625, Total Reward: -20.305639266967773\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0  20]\n"," [  7   0 200]\n"," [  7   0  20]]\n","Reward: -0.5282278060913086\n","Next SOFA: 7.264114\n","Current SOFA: 7\n","False\n","Action: 16, Reward: -0.5282278060913086, Total Reward: -20.833867073059082\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  7   0 200]\n"," [  7   0  20]\n"," [  7   0   0]]\n","Reward: -0.3832588195800781\n","Next SOFA: 7.1916294\n","Current SOFA: 7\n","False\n","Action: 10, Reward: -0.3832588195800781, Total Reward: -21.21712589263916\n","[[ 1 20]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 7  0 20]\n"," [ 7  0  0]\n"," [ 7  1 20]]\n","Reward: -0.3598194122314453\n","Next SOFA: 7.1799097\n","Current SOFA: 7\n","False\n","Action: 21, Reward: -0.3598194122314453, Total Reward: -21.576945304870605\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[ 7  0  0]\n"," [ 7  1 20]\n"," [ 7  0 20]]\n","Reward: -1.1668901443481445\n","Next SOFA: 7.583445\n","Current SOFA: 7\n","False\n","Action: 11, Reward: -1.1668901443481445, Total Reward: -22.74383544921875\n","[[0.1 0. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 7  1 20]\n"," [ 7  0 20]\n"," [ 7  0  0]]\n","Reward: -0.076873779296875\n","Next SOFA: 7.038437\n","Current SOFA: 7\n","False\n","Action: 15, Reward: -0.076873779296875, Total Reward: -22.820709228515625\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  7   0  20]\n"," [  7   0   0]\n"," [  7   0 200]]\n","Reward: -0.1095123291015625\n","Next SOFA: 7.054756\n","Current SOFA: 7\n","False\n","Action: 9, Reward: -0.1095123291015625, Total Reward: -22.930221557617188\n","[[1 0]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0   0]\n"," [  7   0 200]\n"," [  7   1   0]]\n","Reward: -0.6039314270019531\n","Next SOFA: 7.3019657\n","Current SOFA: 7\n","False\n","Action: 20, Reward: -0.6039314270019531, Total Reward: -23.53415298461914\n","[[  0 200]]\n","1/1 [==============================] - 0s 39ms/step\n","Action history: [[  7   0 200]\n"," [  7   1   0]\n"," [  6   0 200]]\n","Reward: 0.2824411392211914\n","Next SOFA: 6.8587794\n","Current SOFA: 7\n","False\n","Action: 4, Reward: 0.2824411392211914, Total Reward: -23.25171184539795\n","[[1 0]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   1   0]\n"," [  6   0 200]\n"," [  7   1   0]]\n","Reward: -3.0359134674072266\n","Next SOFA: 7.5179567\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -3.0359134674072266, Total Reward: -26.287625312805176\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[  6   0 200]\n"," [  7   1   0]\n"," [  6   0  20]]\n","Reward: 0.017789840698242188\n","Next SOFA: 6.991105\n","Current SOFA: 7\n","False\n","Action: 6, Reward: 0.017789840698242188, Total Reward: -26.269835472106934\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   1   0]\n"," [  6   0  20]\n"," [  6   0 100]]\n","Reward: -1.9581918716430664\n","Next SOFA: 6.979096\n","Current SOFA: 6\n","False\n","Action: 8, Reward: -1.9581918716430664, Total Reward: -28.22802734375\n","[[0 0]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0  20]\n"," [  6   0 100]\n"," [  7   0   0]]\n","Reward: -2.2804441452026367\n","Next SOFA: 7.140222\n","Current SOFA: 6\n","False\n","Action: 0, Reward: -2.2804441452026367, Total Reward: -30.508471488952637\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  6   0 100]\n"," [  7   0   0]\n"," [  6   0 200]]\n","Reward: 0.1598491668701172\n","Next SOFA: 6.9200754\n","Current SOFA: 7\n","False\n","Action: 9, Reward: 0.1598491668701172, Total Reward: -30.34862232208252\n","[[1 0]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  7   0   0]\n"," [  6   0 200]\n"," [  6   1   0]]\n","Reward: -1.566019058227539\n","Next SOFA: 6.7830095\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -1.566019058227539, Total Reward: -31.91464138031006\n","[[ 1 20]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 200]\n"," [  6   1   0]\n"," [  6   1  20]]\n","Reward: -1.349151611328125\n","Next SOFA: 6.674576\n","Current SOFA: 6\n","False\n","Action: 21, Reward: -1.349151611328125, Total Reward: -33.263792991638184\n","[[1.e-02 1.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   1   0]\n"," [  6   1  20]\n"," [  6   0 100]]\n","Reward: -1.4741392135620117\n","Next SOFA: 6.7370696\n","Current SOFA: 6\n","False\n","Action: 13, Reward: -1.4741392135620117, Total Reward: -34.737932205200195\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  6   1  20]\n"," [  6   0 100]\n"," [  6   0 200]]\n","Reward: -1.1698017120361328\n","Next SOFA: 6.584901\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.1698017120361328, Total Reward: -35.90773391723633\n","[[1.e-02 2.e+02]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[  6   0 100]\n"," [  6   0 200]\n"," [  6   0 200]]\n","Reward: -1.1534709930419922\n","Next SOFA: 6.5767355\n","Current SOFA: 6\n","False\n","Action: 14, Reward: -1.1534709930419922, Total Reward: -37.06120491027832\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 43ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 200]\n"," [  6   0 200]]\n","Reward: -1.4464845657348633\n","Next SOFA: 6.7232423\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.4464845657348633, Total Reward: -38.507689476013184\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 200]\n"," [  6   0 200]]\n","Reward: -1.6149921417236328\n","Next SOFA: 6.807496\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.6149921417236328, Total Reward: -40.122681617736816\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 200]\n"," [  6   0   0]]\n","Reward: -1.4206132888793945\n","Next SOFA: 6.7103066\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -1.4206132888793945, Total Reward: -41.54329490661621\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  6   0 200]\n"," [  6   0   0]\n"," [  6   0 200]]\n","Reward: -1.0950841903686523\n","Next SOFA: 6.547542\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.0950841903686523, Total Reward: -42.63837909698486\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0   0]\n"," [  6   0 200]\n"," [  6   0 200]]\n","Reward: -1.3427000045776367\n","Next SOFA: 6.67135\n","Current SOFA: 6\n","False\n","Action: 19, Reward: -1.3427000045776367, Total Reward: -43.9810791015625\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 200]\n"," [  6   0   0]]\n","Reward: -1.4494991302490234\n","Next SOFA: 6.7247496\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -1.4494991302490234, Total Reward: -45.43057823181152\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0 200]\n"," [  6   0   0]\n"," [  6   0   0]]\n","Reward: -0.6795854568481445\n","Next SOFA: 6.3397927\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -0.6795854568481445, Total Reward: -46.11016368865967\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[6 0 0]\n"," [6 0 0]\n"," [6 0 0]]\n","Reward: -1.4140825271606445\n","Next SOFA: 6.7070413\n","Current SOFA: 6\n","False\n","Action: 10, Reward: -1.4140825271606445, Total Reward: -47.52424621582031\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0   0]\n"," [  6   0   0]\n"," [  7   0 200]]\n","Reward: -2.150599479675293\n","Next SOFA: 7.0752997\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -2.150599479675293, Total Reward: -49.674845695495605\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0   0]\n"," [  7   0 200]\n"," [  7   0 200]]\n","Reward: -0.22356128692626953\n","Next SOFA: 7.1117806\n","Current SOFA: 7\n","False\n","Action: 9, Reward: -0.22356128692626953, Total Reward: -49.898406982421875\n","[[ 0 20]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   0 200]\n"," [  7   0 200]\n"," [  6   0  20]]\n","Reward: 0.13889503479003906\n","Next SOFA: 6.9305525\n","Current SOFA: 7\n","False\n","Action: 1, Reward: 0.13889503479003906, Total Reward: -49.759511947631836\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 37ms/step\n","Action history: [[  7   0 200]\n"," [  6   0  20]\n"," [  7   0   0]]\n","Reward: -2.106144905090332\n","Next SOFA: 7.0530725\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -2.106144905090332, Total Reward: -51.86565685272217\n","[[ 1 60]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[ 6  0 20]\n"," [ 7  0  0]\n"," [ 7  1 60]]\n","Reward: -0.6040143966674805\n","Next SOFA: 7.302007\n","Current SOFA: 7\n","False\n","Action: 22, Reward: -0.6040143966674805, Total Reward: -52.46967124938965\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  7   0   0]\n"," [  7   1  60]\n"," [  6   0 100]]\n","Reward: 0.12790679931640625\n","Next SOFA: 6.9360466\n","Current SOFA: 7\n","False\n","Action: 8, Reward: 0.12790679931640625, Total Reward: -52.34176445007324\n","[[1.e-02 6.e+01]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  7   1  60]\n"," [  6   0 100]\n"," [  7   0  60]]\n","Reward: -2.4830245971679688\n","Next SOFA: 7.2415123\n","Current SOFA: 6\n","False\n","Action: 12, Reward: -2.4830245971679688, Total Reward: -54.82478904724121\n","[[ 1 60]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  6   0 100]\n"," [  7   0  60]\n"," [  7   1  60]]\n","Reward: -0.2356119155883789\n","Next SOFA: 7.117806\n","Current SOFA: 7\n","False\n","Action: 22, Reward: -0.2356119155883789, Total Reward: -55.06040096282959\n","[[1.e-02 6.e+01]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 7  0 60]\n"," [ 7  1 60]\n"," [ 6  0 60]]\n","Reward: 0.28623008728027344\n","Next SOFA: 6.856885\n","Current SOFA: 7\n","False\n","Action: 12, Reward: 0.28623008728027344, Total Reward: -54.774170875549316\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[ 7  1 60]\n"," [ 6  0 60]\n"," [ 6  0 20]]\n","Reward: -1.9947500228881836\n","Next SOFA: 6.997375\n","Current SOFA: 6\n","False\n","Action: 6, Reward: -1.9947500228881836, Total Reward: -56.7689208984375\n","[[0 0]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[ 6  0 60]\n"," [ 6  0 20]\n"," [ 6  0  0]]\n","Reward: -1.4127483367919922\n","Next SOFA: 6.706374\n","Current SOFA: 6\n","False\n","Action: 0, Reward: -1.4127483367919922, Total Reward: -58.18166923522949\n","[[ 0.1 60. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 6  0 20]\n"," [ 6  0  0]\n"," [ 6  0 60]]\n","Reward: -1.1624774932861328\n","Next SOFA: 6.5812387\n","Current SOFA: 6\n","False\n","Action: 17, Reward: -1.1624774932861328, Total Reward: -59.344146728515625\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[ 6  0  0]\n"," [ 6  0 60]\n"," [ 6  0 20]]\n","Reward: -1.377028465270996\n","Next SOFA: 6.688514\n","Current SOFA: 6\n","False\n","Action: 11, Reward: -1.377028465270996, Total Reward: -60.72117519378662\n","[[ 0.1 60. ]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[ 6  0 60]\n"," [ 6  0 20]\n"," [ 6  0 60]]\n","Reward: -0.12258052825927734\n","Next SOFA: 6.0612903\n","Current SOFA: 6\n","False\n","Action: 17, Reward: -0.12258052825927734, Total Reward: -60.8437557220459\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[ 6  0 20]\n"," [ 6  0 60]\n"," [ 6  0 20]]\n","Reward: -1.4268884658813477\n","Next SOFA: 6.713444\n","Current SOFA: 6\n","False\n","Action: 6, Reward: -1.4268884658813477, Total Reward: -62.270644187927246\n","[[  0 200]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[  6   0  60]\n"," [  6   0  20]\n"," [  6   0 200]]\n","Reward: -1.5135717391967773\n","Next SOFA: 6.756786\n","Current SOFA: 6\n","False\n","Action: 4, Reward: -1.5135717391967773, Total Reward: -63.78421592712402\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0  20]\n"," [  6   0 200]\n"," [  6   0  20]]\n","Reward: -1.2057628631591797\n","Next SOFA: 6.6028814\n","Current SOFA: 6\n","False\n","Action: 16, Reward: -1.2057628631591797, Total Reward: -64.9899787902832\n","[[  0 100]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  6   0 200]\n"," [  6   0  20]\n"," [  6   0 100]]\n","Reward: -0.9282760620117188\n","Next SOFA: 6.464138\n","Current SOFA: 6\n","False\n","Action: 3, Reward: -0.9282760620117188, Total Reward: -65.91825485229492\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0  20]\n"," [  6   0 100]\n"," [  6   0   0]]\n","Reward: -1.4181957244873047\n","Next SOFA: 6.709098\n","Current SOFA: 6\n","False\n","Action: 10, Reward: -1.4181957244873047, Total Reward: -67.33645057678223\n","[[ 0 60]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 100]\n"," [  6   0   0]\n"," [  6   0  60]]\n","Reward: -0.7248249053955078\n","Next SOFA: 6.3624125\n","Current SOFA: 6\n","False\n","Action: 2, Reward: -0.7248249053955078, Total Reward: -68.06127548217773\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0   0]\n"," [  6   0  60]\n"," [  6   0 200]]\n","Reward: -1.463836669921875\n","Next SOFA: 6.7319183\n","Current SOFA: 6\n","False\n","Action: 19, Reward: -1.463836669921875, Total Reward: -69.52511215209961\n","[[ 1 20]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0  60]\n"," [  6   0 200]\n"," [  6   1  20]]\n","Reward: -1.315800666809082\n","Next SOFA: 6.6579003\n","Current SOFA: 6\n","False\n","Action: 21, Reward: -1.315800666809082, Total Reward: -70.84091281890869\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  6   0 200]\n"," [  6   1  20]\n"," [  6   0  60]]\n","Reward: -0.9046926498413086\n","Next SOFA: 6.4523463\n","Current SOFA: 6\n","False\n","Action: 7, Reward: -0.9046926498413086, Total Reward: -71.74560546875\n","[[1 0]]\n","1/1 [==============================] - 0s 23ms/step\n","Action history: [[ 6  1 20]\n"," [ 6  0 60]\n"," [ 6  1  0]]\n","Reward: -1.5848464965820312\n","Next SOFA: 6.7924232\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -1.5848464965820312, Total Reward: -73.33045196533203\n","[[1 0]]\n","1/1 [==============================] - 0s 27ms/step\n","Action history: [[ 6  0 60]\n"," [ 6  1  0]\n"," [ 6  1  0]]\n","Reward: -0.8374996185302734\n","Next SOFA: 6.41875\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -0.8374996185302734, Total Reward: -74.1679515838623\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 33ms/step\n","Action history: [[6 1 0]\n"," [6 1 0]\n"," [6 0 0]]\n","Reward: -0.3955507278442383\n","Next SOFA: 6.1977754\n","Current SOFA: 6\n","False\n","Action: 10, Reward: -0.3955507278442383, Total Reward: -74.56350231170654\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 38ms/step\n","Action history: [[ 6  1  0]\n"," [ 6  0  0]\n"," [ 6  0 20]]\n","Reward: -1.2701501846313477\n","Next SOFA: 6.635075\n","Current SOFA: 6\n","False\n","Action: 11, Reward: -1.2701501846313477, Total Reward: -75.83365249633789\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 29ms/step\n","Action history: [[ 6  0  0]\n"," [ 6  0 20]\n"," [ 6  0  0]]\n","Reward: -1.9191827774047852\n","Next SOFA: 6.9595914\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -1.9191827774047852, Total Reward: -77.75283527374268\n","[[0 0]]\n","1/1 [==============================] - 0s 40ms/step\n","Action history: [[ 6  0 20]\n"," [ 6  0  0]\n"," [ 6  0  0]]\n","Reward: -0.30548095703125\n","Next SOFA: 6.1527405\n","Current SOFA: 6\n","False\n","Action: 0, Reward: -0.30548095703125, Total Reward: -78.05831623077393\n","[[1.e-02 1.e+02]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  6   0   0]\n"," [  6   0   0]\n"," [  6   0 100]]\n","Reward: -1.4618492126464844\n","Next SOFA: 6.7309246\n","Current SOFA: 6\n","False\n","Action: 13, Reward: -1.4618492126464844, Total Reward: -79.52016544342041\n","[[0 0]]\n","1/1 [==============================] - 0s 33ms/step\n","Action history: [[  6   0   0]\n"," [  6   0 100]\n"," [  7   0   0]]\n","Reward: -2.175929069519043\n","Next SOFA: 7.0879645\n","Current SOFA: 6\n","False\n","Action: 0, Reward: -2.175929069519043, Total Reward: -81.69609451293945\n","[[  0.1 100. ]]\n","1/1 [==============================] - 0s 31ms/step\n","Action history: [[  6   0 100]\n"," [  7   0   0]\n"," [  6   0 100]]\n","Reward: 1.1363420486450195\n","Next SOFA: 6.431829\n","Current SOFA: 7\n","False\n","Action: 18, Reward: 1.1363420486450195, Total Reward: -80.55975246429443\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 34ms/step\n","Action history: [[  7   0   0]\n"," [  6   0 100]\n"," [  6   0  20]]\n","Reward: -1.7129535675048828\n","Next SOFA: 6.856477\n","Current SOFA: 6\n","False\n","Action: 6, Reward: -1.7129535675048828, Total Reward: -82.27270603179932\n","[[  0 200]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  6   0 100]\n"," [  6   0  20]\n"," [  7   0 200]]\n","Reward: -2.3356447219848633\n","Next SOFA: 7.1678224\n","Current SOFA: 6\n","False\n","Action: 4, Reward: -2.3356447219848633, Total Reward: -84.60835075378418\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 32ms/step\n","Action history: [[  6   0  20]\n"," [  7   0 200]\n"," [  6   0  60]]\n","Reward: 0.46518421173095703\n","Next SOFA: 6.767408\n","Current SOFA: 7\n","False\n","Action: 7, Reward: 0.46518421173095703, Total Reward: -84.14316654205322\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  7   0 200]\n"," [  6   0  60]\n"," [  6   0  60]]\n","Reward: -1.663447380065918\n","Next SOFA: 6.8317237\n","Current SOFA: 6\n","False\n","Action: 7, Reward: -1.663447380065918, Total Reward: -85.80661392211914\n","[[  1 100]]\n","1/1 [==============================] - 0s 29ms/step\n","Action history: [[  6   0  60]\n"," [  6   0  60]\n"," [  7   1 100]]\n","Reward: -2.838409423828125\n","Next SOFA: 7.4192047\n","Current SOFA: 6\n","False\n","Action: 23, Reward: -2.838409423828125, Total Reward: -88.64502334594727\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[  6   0  60]\n"," [  7   1 100]\n"," [  6   0 200]]\n","Reward: 0.6492595672607422\n","Next SOFA: 6.67537\n","Current SOFA: 7\n","False\n","Action: 19, Reward: 0.6492595672607422, Total Reward: -87.99576377868652\n","[[  1 200]]\n","1/1 [==============================] - 0s 34ms/step\n","Action history: [[  7   1 100]\n"," [  6   0 200]\n"," [  6   1 200]]\n","Reward: -1.529210090637207\n","Next SOFA: 6.764605\n","Current SOFA: 6\n","False\n","Action: 24, Reward: -1.529210090637207, Total Reward: -89.52497386932373\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 33ms/step\n","Action history: [[  6   0 200]\n"," [  6   1 200]\n"," [  7   0 100]]\n","Reward: -2.363739013671875\n","Next SOFA: 7.1818695\n","Current SOFA: 6\n","False\n","Action: 8, Reward: -2.363739013671875, Total Reward: -91.8887128829956\n","[[1.e-02 6.e+01]]\n","1/1 [==============================] - 0s 60ms/step\n","Action history: [[  6   1 200]\n"," [  7   0 100]\n"," [  6   0  60]]\n","Reward: 0.41451549530029297\n","Next SOFA: 6.7927423\n","Current SOFA: 7\n","False\n","Action: 12, Reward: 0.41451549530029297, Total Reward: -91.47419738769531\n","[[1 0]]\n","1/1 [==============================] - 0s 32ms/step\n","Action history: [[  7   0 100]\n"," [  6   0  60]\n"," [  6   1   0]]\n","Reward: -1.413233757019043\n","Next SOFA: 6.706617\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -1.413233757019043, Total Reward: -92.88743114471436\n","[[ 1 20]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[ 6  0 60]\n"," [ 6  1  0]\n"," [ 7  1 20]]\n","Reward: -2.1202621459960938\n","Next SOFA: 7.060131\n","Current SOFA: 6\n","False\n","Action: 21, Reward: -2.1202621459960938, Total Reward: -95.00769329071045\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 29ms/step\n","Action history: [[  6   1   0]\n"," [  7   1  20]\n"," [  6   0 200]]\n","Reward: 0.6076574325561523\n","Next SOFA: 6.6961713\n","Current SOFA: 7\n","False\n","Action: 9, Reward: 0.6076574325561523, Total Reward: -94.4000358581543\n","[[0.1 0. ]]\n","1/1 [==============================] - 0s 33ms/step\n","Action history: [[  7   1  20]\n"," [  6   0 200]\n"," [  7   0   0]]\n","Reward: -2.1411428451538086\n","Next SOFA: 7.0705714\n","Current SOFA: 6\n","False\n","Action: 15, Reward: -2.1411428451538086, Total Reward: -96.5411787033081\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 28ms/step\n","Action history: [[  6   0 200]\n"," [  7   0   0]\n"," [  7   0  20]]\n","Reward: -0.017271995544433594\n","Next SOFA: 7.008636\n","Current SOFA: 7\n","False\n","Action: 11, Reward: -0.017271995544433594, Total Reward: -96.55845069885254\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[ 7  0  0]\n"," [ 7  0 20]\n"," [ 6  0 20]]\n","Reward: 0.8609275817871094\n","Next SOFA: 6.569536\n","Current SOFA: 7\n","False\n","Action: 16, Reward: 0.8609275817871094, Total Reward: -95.69752311706543\n","[[1 0]]\n","1/1 [==============================] - 0s 37ms/step\n","Action history: [[ 7  0 20]\n"," [ 6  0 20]\n"," [ 6  1  0]]\n","Reward: -1.5752677917480469\n","Next SOFA: 6.787634\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -1.5752677917480469, Total Reward: -97.27279090881348\n","[[1.e-02 1.e+02]]\n","1/1 [==============================] - 0s 32ms/step\n","Action history: [[  6   0  20]\n"," [  6   1   0]\n"," [  6   0 100]]\n","Reward: -1.8282899856567383\n","Next SOFA: 6.914145\n","Current SOFA: 6\n","False\n","Action: 13, Reward: -1.8282899856567383, Total Reward: -99.10108089447021\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 33ms/step\n","Action history: [[  6   1   0]\n"," [  6   0 100]\n"," [  6   0   0]]\n","Reward: -1.5685768127441406\n","Next SOFA: 6.7842884\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -1.5685768127441406, Total Reward: -100.66965770721436\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[  6   0 100]\n"," [  6   0   0]\n"," [  6   0 200]]\n","Reward: -1.168731689453125\n","Next SOFA: 6.584366\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.168731689453125, Total Reward: -101.83838939666748\n","[[ 1 60]]\n","1/1 [==============================] - 0s 32ms/step\n","Action history: [[  6   0   0]\n"," [  6   0 200]\n"," [  6   1  60]]\n","Reward: -1.9608278274536133\n","Next SOFA: 6.980414\n","Current SOFA: 6\n","False\n","Action: 22, Reward: -1.9608278274536133, Total Reward: -103.7992172241211\n","[[  0.1 100. ]]\n","1/1 [==============================] - 0s 32ms/step\n","Action history: [[  6   0 200]\n"," [  6   1  60]\n"," [  6   0 100]]\n","Reward: -1.2396583557128906\n","Next SOFA: 6.619829\n","Current SOFA: 6\n","False\n","Action: 18, Reward: -1.2396583557128906, Total Reward: -105.03887557983398\n","[[  0.1 100. ]]\n","1/1 [==============================] - 0s 31ms/step\n","Action history: [[  6   1  60]\n"," [  6   0 100]\n"," [  6   0 100]]\n","Reward: -1.7978897094726562\n","Next SOFA: 6.898945\n","Current SOFA: 6\n","False\n","Action: 18, Reward: -1.7978897094726562, Total Reward: -106.83676528930664\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[  6   0 100]\n"," [  6   0 100]\n"," [  6   0 200]]\n","Reward: -1.1891365051269531\n","Next SOFA: 6.5945683\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.1891365051269531, Total Reward: -108.0259017944336\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 46ms/step\n","Action history: [[  6   0 100]\n"," [  6   0 200]\n"," [  6   0  60]]\n","Reward: -1.7113895416259766\n","Next SOFA: 6.855695\n","Current SOFA: 6\n","False\n","Action: 7, Reward: -1.7113895416259766, Total Reward: -109.73729133605957\n","[[ 0 60]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[  6   0 200]\n"," [  6   0  60]\n"," [  6   0  60]]\n","Reward: -1.4081611633300781\n","Next SOFA: 6.7040806\n","Current SOFA: 6\n","False\n","Action: 2, Reward: -1.4081611633300781, Total Reward: -111.14545249938965\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 30ms/step\n","Action history: [[ 6  0 60]\n"," [ 6  0 60]\n"," [ 6  0  0]]\n","Reward: -1.6613006591796875\n","Next SOFA: 6.8306503\n","Current SOFA: 6\n","False\n","Action: 10, Reward: -1.6613006591796875, Total Reward: -112.80675315856934\n","[[  0 200]]\n","1/1 [==============================] - 0s 29ms/step\n","Action history: [[  6   0  60]\n"," [  6   0   0]\n"," [  6   0 200]]\n","Reward: -0.7174100875854492\n","Next SOFA: 6.358705\n","Current SOFA: 6\n","False\n","Action: 4, Reward: -0.7174100875854492, Total Reward: -113.52416324615479\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 31ms/step\n","Action history: [[  6   0   0]\n"," [  6   0 200]\n"," [  6   0 200]]\n","Reward: -1.390212059020996\n","Next SOFA: 6.695106\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -1.390212059020996, Total Reward: -114.91437530517578\n","[[ 1 20]]\n","1/1 [==============================] - 0s 35ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 200]\n"," [  6   1  20]]\n","Reward: -1.4337787628173828\n","Next SOFA: 6.7168894\n","Current SOFA: 6\n","False\n","Action: 21, Reward: -1.4337787628173828, Total Reward: -116.34815406799316\n","[[  1 200]]\n","1/1 [==============================] - 0s 31ms/step\n","Action history: [[  6   0 200]\n"," [  6   1  20]\n"," [  6   1 200]]\n","Reward: -1.235041618347168\n","Next SOFA: 6.617521\n","Current SOFA: 6\n","False\n","Action: 24, Reward: -1.235041618347168, Total Reward: -117.58319568634033\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 37ms/step\n","Action history: [[  6   1  20]\n"," [  6   1 200]\n"," [  6   0  20]]\n","Reward: -1.6680879592895508\n","Next SOFA: 6.834044\n","Current SOFA: 6\n","False\n","Action: 11, Reward: -1.6680879592895508, Total Reward: -119.25128364562988\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 31ms/step\n","Action history: [[  6   1 200]\n"," [  6   0  20]\n"," [  6   0   0]]\n","Reward: -0.7208566665649414\n","Next SOFA: 6.3604283\n","Current SOFA: 6\n","False\n","Action: 10, Reward: -0.7208566665649414, Total Reward: -119.97214031219482\n","[[1 0]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[ 6  0 20]\n"," [ 6  0  0]\n"," [ 6  1  0]]\n","Reward: -0.7616748809814453\n","Next SOFA: 6.3808374\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -0.7616748809814453, Total Reward: -120.73381519317627\n","[[0.001 0.   ]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[6 0 0]\n"," [6 1 0]\n"," [6 0 0]]\n","Reward: -1.4367027282714844\n","Next SOFA: 6.7183514\n","Current SOFA: 6\n","False\n","Action: 5, Reward: -1.4367027282714844, Total Reward: -122.17051792144775\n","[[  0 100]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[  6   1   0]\n"," [  6   0   0]\n"," [  6   0 100]]\n","Reward: -1.8048992156982422\n","Next SOFA: 6.9024496\n","Current SOFA: 6\n","False\n","Action: 3, Reward: -1.8048992156982422, Total Reward: -123.975417137146\n","[[1 0]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0   0]\n"," [  6   0 100]\n"," [  7   1   0]]\n","Reward: -2.4358043670654297\n","Next SOFA: 7.217902\n","Current SOFA: 6\n","False\n","Action: 20, Reward: -2.4358043670654297, Total Reward: -126.41122150421143\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 100]\n"," [  7   1   0]\n"," [  6   0 200]]\n","Reward: 0.728001594543457\n","Next SOFA: 6.635999\n","Current SOFA: 7\n","False\n","Action: 19, Reward: 0.728001594543457, Total Reward: -125.68321990966797\n","[[  0.1 100. ]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[  7   1   0]\n"," [  6   0 200]\n"," [  6   0 100]]\n","Reward: -1.6331768035888672\n","Next SOFA: 6.8165884\n","Current SOFA: 6\n","False\n","Action: 18, Reward: -1.6331768035888672, Total Reward: -127.31639671325684\n","[[1.e-03 2.e+02]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 200]\n"," [  6   0 100]\n"," [  7   0 200]]\n","Reward: -2.115668296813965\n","Next SOFA: 7.057834\n","Current SOFA: 6\n","False\n","Action: 9, Reward: -2.115668296813965, Total Reward: -129.4320650100708\n","[[1.e-03 6.e+01]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0 100]\n"," [  7   0 200]\n"," [  6   0  60]]\n","Reward: 0.022207260131835938\n","Next SOFA: 6.9888964\n","Current SOFA: 7\n","False\n","Action: 7, Reward: 0.022207260131835938, Total Reward: -129.40985774993896\n","[[  0 200]]\n","1/1 [==============================] - 0s 27ms/step\n","Action history: [[  7   0 200]\n"," [  6   0  60]\n"," [  6   0 200]]\n","Reward: -1.651169776916504\n","Next SOFA: 6.825585\n","Current SOFA: 6\n","False\n","Action: 4, Reward: -1.651169776916504, Total Reward: -131.06102752685547\n","[[  1 100]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[  6   0  60]\n"," [  6   0 200]\n"," [  7   1 100]]\n","Reward: -2.7161312103271484\n","Next SOFA: 7.3580656\n","Current SOFA: 6\n","False\n","Action: 23, Reward: -2.7161312103271484, Total Reward: -133.77715873718262\n","[[1.e-02 6.e+01]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 200]\n"," [  7   1 100]\n"," [  6   0  60]]\n","Reward: 0.3611927032470703\n","Next SOFA: 6.8194036\n","Current SOFA: 7\n","False\n","Action: 12, Reward: 0.3611927032470703, Total Reward: -133.41596603393555\n","[[ 0.1 60. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   1 100]\n"," [  6   0  60]\n"," [  6   0  60]]\n","Reward: -1.8533868789672852\n","Next SOFA: 6.9266934\n","Current SOFA: 6\n","False\n","Action: 17, Reward: -1.8533868789672852, Total Reward: -135.26935291290283\n","[[1.e-02 1.e+02]]\n","1/1 [==============================] - 0s 19ms/step\n","Action history: [[  6   0  60]\n"," [  6   0  60]\n"," [  7   0 100]]\n","Reward: -2.2661733627319336\n","Next SOFA: 7.1330867\n","Current SOFA: 6\n","False\n","Action: 13, Reward: -2.2661733627319336, Total Reward: -137.53552627563477\n","[[1.e-03 1.e+02]]\n","1/1 [==============================] - 0s 27ms/step\n","Action history: [[  6   0  60]\n"," [  7   0 100]\n"," [  6   0 100]]\n","Reward: 0.39519214630126953\n","Next SOFA: 6.802404\n","Current SOFA: 7\n","False\n","Action: 8, Reward: 0.39519214630126953, Total Reward: -137.1403341293335\n","[[1.e-01 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0 100]\n"," [  6   0 100]\n"," [  6   0 200]]\n","Reward: -1.4138412475585938\n","Next SOFA: 6.7069206\n","Current SOFA: 6\n","False\n","Action: 19, Reward: -1.4138412475585938, Total Reward: -138.5541753768921\n","[[ 0 20]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  6   0 100]\n"," [  6   0 200]\n"," [  7   0  20]]\n","Reward: -2.4210338592529297\n","Next SOFA: 7.210517\n","Current SOFA: 6\n","False\n","Action: 1, Reward: -2.4210338592529297, Total Reward: -140.97520923614502\n","[[1.e-02 2.e+01]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0 200]\n"," [  7   0  20]\n"," [  6   0  20]]\n","Reward: 0.3167877197265625\n","Next SOFA: 6.841606\n","Current SOFA: 7\n","False\n","Action: 11, Reward: 0.3167877197265625, Total Reward: -140.65842151641846\n","[[ 0.1 20. ]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[ 7  0 20]\n"," [ 6  0 20]\n"," [ 7  0 20]]\n","Reward: -2.019258499145508\n","Next SOFA: 7.0096292\n","Current SOFA: 6\n","False\n","Action: 16, Reward: -2.019258499145508, Total Reward: -142.67768001556396\n","[[1 0]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[ 6  0 20]\n"," [ 7  0 20]\n"," [ 7  1  0]]\n","Reward: -0.2312936782836914\n","Next SOFA: 7.115647\n","Current SOFA: 7\n","False\n","Action: 20, Reward: -0.2312936782836914, Total Reward: -142.90897369384766\n","[[  0 200]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   0  20]\n"," [  7   1   0]\n"," [  6   0 200]]\n","Reward: 0.03544807434082031\n","Next SOFA: 6.982276\n","Current SOFA: 7\n","False\n","Action: 4, Reward: 0.03544807434082031, Total Reward: -142.87352561950684\n","[[  0.1 100. ]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[  7   1   0]\n"," [  6   0 200]\n"," [  7   0 100]]\n","Reward: -3.2273635864257812\n","Next SOFA: 7.613682\n","Current SOFA: 6\n","False\n","Action: 18, Reward: -3.2273635864257812, Total Reward: -146.10088920593262\n","[[1.e-02 2.e+02]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  6   0 200]\n"," [  7   0 100]\n"," [  7   0 200]]\n","Reward: -0.06222057342529297\n","Next SOFA: 7.0311103\n","Current SOFA: 7\n","False\n","Action: 14, Reward: -0.06222057342529297, Total Reward: -146.1631097793579\n","[[ 0 60]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0 100]\n"," [  7   0 200]\n"," [  7   0  60]]\n","Reward: -0.19296741485595703\n","Next SOFA: 7.0964837\n","Current SOFA: 7\n","False\n","Action: 2, Reward: -0.19296741485595703, Total Reward: -146.35607719421387\n","[[1.e-02 1.e+02]]\n","1/1 [==============================] - 0s 22ms/step\n","Action history: [[  7   0 200]\n"," [  7   0  60]\n"," [  7   0 100]]\n","Reward: -0.6432905197143555\n","Next SOFA: 7.3216453\n","Current SOFA: 7\n","False\n","Action: 13, Reward: -0.6432905197143555, Total Reward: -146.99936771392822\n","[[ 0 60]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0  60]\n"," [  7   0 100]\n"," [  7   0  60]]\n","Reward: -0.6420831680297852\n","Next SOFA: 7.3210416\n","Current SOFA: 7\n","False\n","Action: 2, Reward: -0.6420831680297852, Total Reward: -147.641450881958\n","[[  1 100]]\n","1/1 [==============================] - 0s 20ms/step\n","Action history: [[  7   0 100]\n"," [  7   0  60]\n"," [  7   1 100]]\n","Reward: -0.48358726501464844\n","Next SOFA: 7.2417936\n","Current SOFA: 7\n","False\n","Action: 23, Reward: -0.48358726501464844, Total Reward: -148.12503814697266\n","[[  1 200]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   0  60]\n"," [  7   1 100]\n"," [  7   1 200]]\n","Reward: -0.494476318359375\n","Next SOFA: 7.247238\n","Current SOFA: 7\n","False\n","Action: 24, Reward: -0.494476318359375, Total Reward: -148.61951446533203\n","[[  1 200]]\n","1/1 [==============================] - 0s 26ms/step\n","Action history: [[  7   1 100]\n"," [  7   1 200]\n"," [  7   1 200]]\n","Reward: -0.8618955612182617\n","Next SOFA: 7.430948\n","Current SOFA: 7\n","False\n","Action: 24, Reward: -0.8618955612182617, Total Reward: -149.4814100265503\n","[[0.01 0.  ]]\n","1/1 [==============================] - 0s 21ms/step\n","Action history: [[  7   1 200]\n"," [  7   1 200]\n"," [  7   0   0]]\n","Reward: -0.5788898468017578\n","Next SOFA: 7.289445\n","Current SOFA: 7\n","False\n","Action: 10, Reward: -0.5788898468017578, Total Reward: -150.06029987335205\n","[[0 0]]\n","1/1 [==============================] - 0s 27ms/step\n","Action history: [[  7   1 200]\n"," [  7   0   0]\n"," [  7   0   0]]\n","Reward: -0.29874134063720703\n","Next SOFA: 7.1493707\n","Current SOFA: 7\n","False\n","Action: 0, Reward: -0.29874134063720703, Total Reward: -150.35904121398926\n","[[0 0]]\n","1/1 [==============================] - 0s 25ms/step\n","Action history: [[7 0 0]\n"," [7 0 0]\n"," [7 0 0]]\n","Reward: -0.33593273162841797\n","Next SOFA: 7.1679664\n","Current SOFA: 7\n","False\n","Action: 0, Reward: -0.33593273162841797, Total Reward: -150.69497394561768\n","[[1.e-03 2.e+01]]\n","1/1 [==============================] - 0s 24ms/step\n","Action history: [[ 7  0  0]\n"," [ 7  0  0]\n"," [ 5  0 20]]\n","Reward: -7.858572006225586\n","Next SOFA: 5.929286\n","Current SOFA: 7\n","True\n","Action: 6, Reward: -7.858572006225586, Total Reward: -158.55354595184326\n","Total Reward after episode: -158.55354595184326\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvDxLF3ZNvNr","executionInfo":{"status":"ok","timestamp":1724063896881,"user_tz":-330,"elapsed":21468,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}},"outputId":"ad380b2c-50ab-4b61-a721-a2374c83b11d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}