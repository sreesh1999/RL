{"cells":[{"cell_type":"markdown","metadata":{"id":"6YRC-w_jdrS5"},"source":["Define the Environment"]},{"cell_type":"code","source":["!pip install tensorflow==2.15.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qg4VDqG4557I","outputId":"3e9b590e-e88e-45a3-d69d-2579b8519f17","executionInfo":{"status":"ok","timestamp":1724066493018,"user_tz":-330,"elapsed":101979,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.15.0\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n","  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.16.0\n","    Uninstalling wrapt-1.16.0:\n","      Successfully uninstalled wrapt-1.16.0\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.0\n","    Uninstalling ml-dtypes-0.4.0:\n","      Successfully uninstalled ml-dtypes-0.4.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JrDrcyguyX4h","outputId":"b82accd9-35cc-44b2-e7f5-48df4f0b587e","executionInfo":{"status":"ok","timestamp":1724066515057,"user_tz":-330,"elapsed":22071,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["**Sepsis Environment**"],"metadata":{"id":"rd0jrZD8XiXQ"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"OvYX5Z--dsf4","executionInfo":{"status":"ok","timestamp":1724066651381,"user_tz":-330,"elapsed":5352,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","# Define the columns for state features and action features\n","state_cols = ['SOFA']  # Example state columns\n","action_cols = ['MaxVaso', 'Input4H']  # Medication columns\n","\n","# Function to predict medication effects based on the model\n","def predict_medication_effects(model, state, iv_fluid_dosage, vp_dosage, history):\n","    current_state = state[state_cols].values.reshape(1, -1)  # Shape: (1, 1) if 'SOFA' is the only state feature\n","    action = np.array([vp_dosage, iv_fluid_dosage]).reshape(1, -1)  # Shape: (1, 2)\n","\n","    # Concatenate current state and action into a single input array\n","    model_input = np.concatenate([current_state, action], axis=1)  # Shape: (1, 3)\n","\n","    # Concatenate historical cases with current input\n","    model_input = np.concatenate([model_input, history.reshape(1, -1)], axis=1)  # Shape: (1, 12)\n","\n","    # Predict the next state changes\n","    state_change = model.predict(model_input)\n","\n","    # Update the state with the predicted changes\n","    next_state = state.copy()\n","    next_state[state_cols] += state_change[0]\n","\n","    return next_state, state_change[0][0]\n","\n","# Environment Setup\n","class SepsisEnv(gym.Env):\n","    def __init__(self, dataset_path, model_path):\n","        super(SepsisEnv, self).__init__()\n","        self.dataset = pd.read_csv(dataset_path)  # Load the dataset\n","        self.model = tf.keras.models.load_model(model_path)  # Load the trained model\n","        self.current_index = 0\n","        self.history_size = 3  # Size of history to maintain\n","        self.action_history = np.array([[2, 0, 0], [3, 0, 0], [9, 0, 0]])  # Initialize with example history\n","        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([1, 200]), dtype=np.float32)  # Continuous action space\n","\n","        # Calculate observation space size\n","        self.observation_size = len(state_cols) + len(action_cols) + len(state_cols) * self.history_size\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.observation_size,), dtype=np.float32)\n","\n","    def reset(self):\n","        self.current_index = np.random.randint(0, len(self.dataset))\n","        self.action_history = np.array([[2, 0, 0], [3, 0, 0], [9, 0, 0]])  # Reset to example history\n","        return self._get_observation()\n","\n","    def _get_observation(self):\n","        state = self.dataset.iloc[self.current_index][state_cols].values\n","        current_action = self.dataset.iloc[self.current_index][action_cols].values\n","        recent_history = self.action_history.flatten()  # Use action history\n","\n","        return np.concatenate((state, current_action, recent_history))\n","\n","    def step(self, action):\n","        state = self.dataset.iloc[self.current_index].copy()\n","        next_state = state.copy()\n","\n","        iv_fluid_dosage = action[1]\n","        vp_dosage = action[0]\n","\n","        next_state, sofa_change = predict_medication_effects(self.model, next_state, iv_fluid_dosage, vp_dosage, self.action_history)\n","\n","        current_sofa = self.action_history[2][0]\n","        next_sofa = sofa_change\n","\n","        done = next_sofa >= 25 or next_sofa <= 5\n","\n","        reward = self.calculate_reward(current_sofa, next_sofa)\n","\n","        # Update action history with current action (vp_dosage, iv_fluid_dosage)\n","        self.action_history = np.roll(self.action_history, -1, axis=0)  # Remove the oldest entry\n","        self.action_history[-1] = [sofa_change, vp_dosage, iv_fluid_dosage]\n","\n","        self.current_index += 1\n","\n","        # Prepare the observation to return\n","        observation = self._get_observation()\n","\n","        # Return observation, reward, done status, and info dictionary\n","        info = {'predicted_sofa_state': next_state, 'action_applied': action}\n","        return observation, reward, done, info\n","\n","    def calculate_reward(self, current_sofa, next_sofa):\n","        # Immediate reward based on SOFA score change\n","        if next_sofa < current_sofa:\n","            sofa_reward = (current_sofa - next_sofa) * 2\n","        elif next_sofa > current_sofa:\n","            sofa_reward = (next_sofa - current_sofa) * -2\n","        else:\n","            sofa_reward = 1  # Small reward for maintaining the SOFA score\n","\n","        # Terminal reward based on episode end and SOFA score\n","        if next_sofa <= 5:\n","            terminal_reward = 20  # High reward for achieving a low SOFA score at the end\n","        else:\n","            terminal_reward = -10  # Penalty for high SOFA score at the end\n","\n","        return sofa_reward + terminal_reward\n"]},{"cell_type":"markdown","metadata":{"id":"7ZZXXucNd8QL"},"source":["**DDPG Agent**"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"K14ffZkld8tR","executionInfo":{"status":"ok","timestamp":1724066704628,"user_tz":-330,"elapsed":318,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"outputs":[],"source":["import numpy as np\n","import random\n","from tensorflow.keras.layers import Dense, Input, concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","# DDPG Agent Class\n","class DDPGAgent:\n","    def __init__(self, env, state_dim, action_dim, action_high):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.action_high = action_high\n","        self.action_low = 0\n","\n","        self.actor = self.build_actor()\n","        self.actor_target = self.build_actor()\n","        self.actor_target.set_weights(self.actor.get_weights())\n","\n","        self.critic = self.build_critic()\n","        self.critic_target = self.build_critic()\n","        self.critic_target.set_weights(self.critic.get_weights())\n","\n","        self.actor_optimizer = Adam(learning_rate=0.001)\n","        self.critic_optimizer = Adam(learning_rate=0.002)\n","\n","        self.replay_buffer = ReplayBuffer()\n","\n","    def build_actor(self):\n","        state_input = Input(shape=(self.state_dim,))\n","        x = Dense(32, activation='relu')(state_input)\n","        x = Dense(16, activation='relu')(x)\n","        action_output = Dense(self.action_dim, activation='tanh')(x)\n","        scaled_output = tf.multiply(action_output, self.action_high)\n","        model = Model(inputs=state_input, outputs=scaled_output)\n","        return model\n","\n","    def build_critic(self):\n","        state_input = Input(shape=(self.state_dim,))\n","        action_input = Input(shape=(self.action_dim,))\n","        x = concatenate([state_input, action_input])\n","        x = Dense(32, activation='relu')(x)\n","        x = Dense(16, activation='relu')(x)\n","        q_value = Dense(1)(x)\n","        model = Model(inputs=[state_input, action_input], outputs=q_value)\n","        return model\n","\n","    def update_target_networks(self, tau=0.005):\n","        self.update_target(self.actor_target.variables, self.actor.variables, tau)\n","        self.update_target(self.critic_target.variables, self.critic.variables, tau)\n","\n","    def update_target(self, target_vars, source_vars, tau):\n","        for target_var, source_var in zip(target_vars, source_vars):\n","            target_var.assign(tau * source_var + (1 - tau) * target_var)\n","    def get_action(self, state):\n","        state = np.reshape(state, [2, self.state_dim])\n","        return self.actor.predict(state)[0]\n","\n","\n","    def train(self, batch_size):\n","        if len(self.replay_buffer) < batch_size:\n","            return\n","\n","        minibatch = self.replay_buffer.sample(batch_size)\n","\n","\n","    # Convert all elements to numpy arrays and ensure correct shapes\n","        states = np.array([np.array(m[0]).reshape(-1) for m in minibatch])\n","        actions = np.array([np.array(m[1]).reshape(-1) for m in minibatch])\n","        rewards = np.array([np.array(m[2]).reshape(1) for m in minibatch])\n","        next_states = np.array([np.array(m[3]).reshape(-1) for m in minibatch])\n","        dones = np.array([np.array(m[4]).reshape(1) for m in minibatch])\n","\n","    # Ensure correct final shapes\n","        states = states.reshape(batch_size, -1)  # (batch_size, state_dim)\n","        actions = actions.reshape(batch_size, -1)  # (batch_size, action_dim)\n","        rewards = rewards.reshape(batch_size, 1)  # (batch_size, 1)\n","        next_states = next_states.reshape(batch_size, -1)  # (batch_size, state_dim)\n","        dones = dones.reshape(batch_size, 1)  # (batch_size, 1)\n","\n","    # Predict the next actions and Q-values\n","        next_actions = self.actor_target.predict(next_states)\n","        q_values = self.critic_target.predict([next_states, next_actions])\n","\n","    # Compute the target Q-values\n","        targets = rewards + (1 - dones) * 0.99 * q_values  #Bell man Equation,here Discount factor is 0.99\n","\n","    # Debugging print statements to check final shapes before training\n","        print(f\"States shape: {states.shape}\")\n","        print(f\"Actions shape: {actions.shape}\")\n","        print(f\"Rewards shape: {rewards.shape}\")\n","        print(f\"Next states shape: {next_states.shape}\")\n","        print(f\"Dones shape: {dones.shape}\")\n","        print(f\"Next actions shape: {next_actions.shape}\")\n","        print(f\"Q-values shape: {q_values.shape}\")\n","        print(f\"Targets shape: {targets.shape}\")\n","\n","    # Train the critic network\n","        self.critic.train_on_batch([states, actions], targets)\n","\n","        with tf.GradientTape() as tape:\n","            predicted_actions = self.actor(states)\n","            critic_value = self.critic([states, predicted_actions])\n","            actor_loss = -tf.math.reduce_mean(critic_value)\n","\n","        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n","\n","        self.update_target_networks()\n","\n","class ReplayBuffer:\n","    def __init__(self, buffer_size=10000):\n","        self.buffer = []\n","        self.buffer_size = buffer_size\n","\n","    def add(self, state, action, reward, next_state, done):\n","        if len(self.buffer) >= self.buffer_size:\n","            self.buffer.pop(0)\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.buffer, batch_size)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n"]},{"cell_type":"markdown","metadata":{"id":"AyadRRWyeL0C"},"source":["Training loop and Plotting"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pmaMgmrXdezq","outputId":"10ac7d07-f958-45a8-d853-61b3baf6c745","executionInfo":{"status":"error","timestamp":1724066809434,"user_tz":-330,"elapsed":11178,"user":{"displayName":"sreelakshmi rajeevan","userId":"14778784693559924029"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"]},{"output_type":"stream","name":"stdout","text":["state_size 6\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 88ms/step\n","episode: 0/170, score: 0, e: 0.001\n","Episode 0: Reward = 0, Avg Reward = 0.0\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","episode: 1/170, score: 0, e: 0.001\n","Episode 1: Reward = 0, Avg Reward = 0.0\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","episode: 2/170, score: 0, e: 0.001\n","Episode 2: Reward = 0, Avg Reward = 0.0\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","episode: 3/170, score: 1, e: 0.001\n","Episode 3: Reward = -2.0566234588623047, Avg Reward = -0.5141558647155762\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","episode: 4/170, score: 1, e: 0.001\n","Episode 4: Reward = -2.436351776123047, Avg Reward = -0.8985950469970703\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 30ms/step\n","episode: 5/170, score: 0, e: 0.001\n","Episode 5: Reward = 0, Avg Reward = -0.748829205830892\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","episode: 6/170, score: 1, e: 0.001\n","Episode 6: Reward = -2.0666255950927734, Avg Reward = -0.9370858328683036\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 26ms/step\n","episode: 7/170, score: 0, e: 0.001\n","Episode 7: Reward = 0, Avg Reward = -0.8199501037597656\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","episode: 8/170, score: 0, e: 0.001\n","Episode 8: Reward = 0, Avg Reward = -0.7288445366753472\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 32ms/step\n","episode: 9/170, score: 1, e: 0.001\n","Episode 9: Reward = -2.273861885070801, Avg Reward = -0.8833462715148925\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","episode: 10/170, score: 1, e: 0.001\n","Episode 10: Reward = -2.316417694091797, Avg Reward = -1.1149880409240722\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","episode: 11/170, score: 1, e: 0.001\n","Episode 11: Reward = -2.273861885070801, Avg Reward = -1.3423742294311523\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","episode: 12/170, score: 0, e: 0.001\n","Episode 12: Reward = 0, Avg Reward = -1.3423742294311523\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 27ms/step\n","episode: 13/170, score: 0, e: 0.001\n","Episode 13: Reward = 0, Avg Reward = -1.136711883544922\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","episode: 14/170, score: 0, e: 0.001\n","Episode 14: Reward = 0, Avg Reward = -0.8930767059326172\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","episode: 15/170, score: 0, e: 0.001\n","Episode 15: Reward = 0, Avg Reward = -0.8930767059326172\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","episode: 16/170, score: 0, e: 0.001\n","Episode 16: Reward = 0, Avg Reward = -0.6864141464233399\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 53ms/step\n","episode: 17/170, score: 0, e: 0.001\n","Episode 17: Reward = 0, Avg Reward = -0.6864141464233399\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 28ms/step\n","episode: 18/170, score: 1, e: 0.001\n","Episode 18: Reward = -2.411041259765625, Avg Reward = -0.9275182723999024\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 30ms/step\n","episode: 19/170, score: 0, e: 0.001\n","Episode 19: Reward = 0, Avg Reward = -0.7001320838928222\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 51ms/step\n","episode: 20/170, score: 0, e: 0.001\n","Episode 20: Reward = 0, Avg Reward = -0.4684903144836426\n","1/1 [==============================] - 0s 54ms/step\n","1/1 [==============================] - 0s 45ms/step\n","episode: 21/170, score: 0, e: 0.001\n","Episode 21: Reward = 0, Avg Reward = -0.2411041259765625\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 68ms/step\n","episode: 22/170, score: 0, e: 0.001\n","Episode 22: Reward = 0, Avg Reward = -0.2411041259765625\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 72ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 50ms/step\n","episode: 23/170, score: 1, e: 0.001\n","Episode 23: Reward = -2.274123191833496, Avg Reward = -0.4685164451599121\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n"]},{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-4038e4222ce8>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-0d15dc698b56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Convert all elements to numpy arrays and ensure correct shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part."]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Assuming you have defined the SepsisEnv and DDPGAgent classes\n","# and they are imported or defined elsewhere in your code.\n","\n","# Load dataset and model path\n","dataset_path = \"/content/drive/MyDrive/RL project/Dataset.csv\"\n","model_path = \"/content/drive/MyDrive/RL project/predict_state_model.keras\"\n","\n","# Initialize environment and DDPG agent\n","env = SepsisEnv(dataset_path, model_path)\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.shape[0]\n","action_bound = 1  # Define according to your action space scaling\n","\n","agent = DDPGAgent(env,state_size, action_size, action_bound)\n","\n","\n","# Train the agent\n","batch_size = 32\n","episodes = 170  # Use the number of rows in the subset as episodes\n","rewards = []\n","avg_rewards = []\n","print('state_size',state_size)\n","\n","for episode in range(episodes):\n","    state = env.reset()\n","    state = np.reshape(state, [2, state_size])\n","    episode_reward = 0\n","\n","    for time in range(40):\n","        action = agent.get_action(state)\n","        discrete_action = np.argmax(action)  # Assuming 'action' is a probability distribution over actions\n","        next_state, reward, done, info = env.step(action)\n","         # Reshape next_state to be consistent with state\n","        # next_state = np.reshape(next_state, (1, state_size))\n","        agent.replay_buffer.add(state[0], action, reward, next_state[0], done)\n","        state = next_state\n","\n","\n","\n","        if done:\n","            print(f\"episode: {episode}/{episodes}, score: {time}, e: {agent.actor_optimizer.learning_rate.numpy():.2}\")\n","            break\n","        if len(agent.replay_buffer.buffer) > batch_size:\n","            agent.train(batch_size)\n","\n","        state = next_state\n","        episode_reward += reward\n","\n","        if done:\n","            break\n","\n","    rewards.append(episode_reward)\n","    avg_rewards.append(np.mean(rewards[-10:]))\n","\n","    print(f\"Episode {episode}: Reward = {episode_reward}, Avg Reward = {np.mean(rewards[-10:])}\")\n","\n","# Plotting\n","plt.plot(rewards)\n","plt.plot(avg_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.title('DDPG Training Rewards')\n","plt.legend(['Reward', 'Avg Reward (last 10 episodes)'])\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}